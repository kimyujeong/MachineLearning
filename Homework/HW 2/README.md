
# HW 2

Implement a Neural Network with Regularization
* Configure the network as input layer, 2 hidden layers, and output layers.
* The first and second hidden layers must have 20 and 7 hidden units, respectively.
* activation function of hidden layer is ReLU.
* The activation function of output layer is sigmoid.
